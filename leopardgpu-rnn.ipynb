{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9022641,"sourceType":"datasetVersion","datasetId":5427593},{"sourceType":"datasetVersion","sourceId":9189570,"datasetId":5555080}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport openslide\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def percentage(mask):\n    return (np.sum(mask > 0) / mask.size) * 100\n\ndef extract_patches(im_slide, ms_slide, level, size_mask, num_patches_needed):\n    f = int(ms_slide.level_downsamples[level])\n    size_scale = im_slide.level_dimensions[level][0] // ms_slide.level_dimensions[level][0]\n    coord_scale = im_slide.level_dimensions[0][0] // ms_slide.level_dimensions[level][0]\n    size_image = (size_mask[0] * size_scale, size_mask[1] * size_scale)\n    \n    ms_width, ms_height = ms_slide.level_dimensions[level]\n    \n    l = [(x_ms, y_ms) for y_ms in range(0, ms_height, size_mask[1]) \n         for x_ms in range(0, ms_width, size_mask[0])]\n    \n    count, used_indices = 0, []\n    random.seed(42)\n    image_patches = []\n\n    while count < num_patches_needed:\n        index = random.randint(0, len(l) - 1)\n        if index not in used_indices:\n            used_indices.append(index)\n            x_ms, y_ms = l[index]\n            x_im, y_im = x_ms * coord_scale, y_ms * coord_scale\n            mask_patch = ms_slide.read_region((x_ms * f, y_ms * f), level, size_mask).convert(\"L\")\n            image_patch = im_slide.read_region((x_im, y_im), level, size_image).convert(\"RGB\")\n   \n            if percentage(np.array(mask_patch)) > 60:\n                image_patches.append(np.array(image_patch))\n                count += 1\n                if count == num_patches_needed:\n                    break\n\n    return image_patches\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# DATASET\nclass PatchDataset(Dataset):\n    def __init__(self, images_dir, masks_dir, csv_file, num_patches_per_image, level, size_mask):\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.df = pd.read_csv(csv_file)\n        self.num_patches_per_image = num_patches_per_image\n        self.level = level\n        self.size_mask = size_mask\n        self.image_list = sorted(os.listdir(images_dir))\n    \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, idx):\n        image_file = self.image_list[idx]\n        impath = os.path.join(self.images_dir, image_file)\n        mspath = os.path.join(self.masks_dir, image_file.replace('.tif', '_tissue.tif'))\n        \n        im_slide = openslide.OpenSlide(impath)\n        ms_slide = openslide.OpenSlide(mspath)\n        \n        image_patches = extract_patches(im_slide, ms_slide, self.level, self.size_mask, self.num_patches_per_image)\n        image_patches = np.array(image_patches, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n        \n        case_id_to_find = image_file\n        filtered_row = self.df.loc[self.df['case_id'] == case_id_to_find].iloc[0]\n#         event = filtered_row[\"event\"]\n        years = filtered_row[\"follow_up_years\"]\n        \n        labels = np.array([[ years]] * self.num_patches_per_image, dtype=np.float32)\n\n        return torch.from_numpy(image_patches.transpose(0, 3, 1, 2)), torch.from_numpy(labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\n\nclass ResNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(ResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out\n\nclass ResNetTransformer(nn.Module):\n    def __init__(self, num_patches, input_shape):\n        super(ResNetTransformer, self).__init__()\n        self.num_patches = num_patches\n        self.input_shape = input_shape\n        \n        # ResNet backbone\n        self.conv1 = nn.Conv2d(input_shape[0], 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.layer1 = self._make_layer(64, 64, 2)\n        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        \n        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        \n        # Final layers\n        self.fc = nn.Linear(512, 64)\n        self.output = nn.Linear(64, 1)  # Changed to output 1 dimension instead of 2\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n        layers = []\n        layers.append(ResNetBlock(in_channels, out_channels, stride))\n        for _ in range(1, num_blocks):\n            layers.append(ResNetBlock(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = x.view(-1, self.input_shape[0], self.input_shape[1], self.input_shape[2])\n        \n        # ResNet backbone\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.view(batch_size, self.num_patches, -1)\n        \n        # Transformer Encoder\n        x = self.transformer_encoder(x)\n        \n        # Final layers\n        x = self.fc(x)\n        x = self.relu(x)\n        x = self.output(x)\n        \n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_loss(y_pred, y_true):\n    mse = nn.MSELoss()\n    event_loss = mse(y_pred[:, :, 0], y_true[:, :, 0])\n    return event_loss \n\n# TRAINING\ndef train_model(model, train_loader, optimizer, device, num_epochs, checkpoint_dir):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = custom_loss(output, target)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            del data , target , output , loss\n            gc.collect()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n\n        if epoch  % 10 == 0:\n            torch.save(model.state_dict(), f\"{checkpoint_dir}/epoch{epoch:5f}-loss{total_loss:.5f}.pth\")\n            print(f\"Model saved at epoch {epoch + 1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 7\nNUM_PATCHES = 33\nINPUT_SHAPE = (3, 512, 512)\nTOTAL_IMAGES = 55\n\nimages_dir = \"/kaggle/input/dddddddd/images\"\nmasks_dir = \"/kaggle/input/dddddddd/masks\"\ncsv_file = \"/kaggle/input/fuckthis/training_labels.csv\"\ncheckpoint_dir = \"/kaggle/working\"\ndataset = PatchDataset(images_dir, masks_dir, csv_file, num_patches_per_image=NUM_PATCHES, level=1, size_mask=(64, 64))\ntrain_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNetTransformer(num_patches=NUM_PATCHES, input_shape=INPUT_SHAPE)\nmodel = nn.DataParallel(model).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x , y in train_loader:\n    print(x.shape , y.shape)\n    break\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1\ntrain_model(model, train_loader, optimizer, device, num_epochs, checkpoint_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # Usage\n# BATCH_SIZE = 16\n# NUM_PATCHES = 4\n# INPUT_SHAPE = (3, 512, 512)\n\n# # Create the model\n# model = ResNetTransformer(num_patches=NUM_PATCHES, input_shape=INPUT_SHAPE)\n\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = nn.DataParallel(model)\n\n# model = model.to(device)\n\n# sample_input = torch.randn(BATCH_SIZE, NUM_PATCHES, *INPUT_SHAPE, device=device)\n\n# output = model(sample_input)\n\n# print(f\"Input shape: {sample_input.shape}\")\n# print(f\"Output shape: {output.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:48.552055Z","iopub.status.idle":"2024-08-17T06:52:48.552575Z","shell.execute_reply.started":"2024-08-17T06:52:48.552305Z","shell.execute_reply":"2024-08-17T06:52:48.552327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **INFERENCE**","metadata":{}},{"cell_type":"code","source":"        impath = os.path.join(self.images_dir, image_file)\n        mspath = os.path.join(self.masks_dir, image_file.replace('.tif', '_tissue.tif'))\n        \n        im_slide = openslide.OpenSlide(impath)\n        ms_slide = openslide.OpenSlide(mspath)\n        \n        image_patches = extract_patches(im_slide, ms_slide, self.level, self.size_mask, self.num_patches_per_image)\n        image_patches = np.array(image_patches, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n                \n\n        return torch.from_numpy(image_patches.transpose(0, 3, 1, 2))","metadata":{},"execution_count":null,"outputs":[]}]}